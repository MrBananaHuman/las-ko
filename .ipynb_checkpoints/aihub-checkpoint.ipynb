{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.set_device(2)\n",
    "torch.manual_seed(7)\n",
    "torch.cuda.manual_seed_all(7)\n",
    "\n",
    "class BLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.0, n_layers=1, bidirectional=True):\n",
    "        super(BLSTM, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.input_dim << 1,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.n_layers,\n",
    "            bidirectional = self.bidirectional,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs, tuple):\n",
    "            inputs, hc = inputs\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len = inputs.size(1)\n",
    "        input_size = inputs.size(2)\n",
    "        if seq_len % 2:\n",
    "            zeros = torch.zeros((inputs.size(0), 1, inputs.size(2))).cuda()\n",
    "            inputs = torch.cat([inputs, zeros], dim = 1)\n",
    "            seq_len += 1\n",
    "        inputs = inputs.contiguous().view(batch_size, int(seq_len / 2), input_size * 2)\n",
    "        \n",
    "        output, hc = self.rnn(inputs, hc)\n",
    "        return (output, hc)\n",
    "\n",
    "\n",
    "class Listener(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.0, n_layers=1, bidirectional=True):\n",
    "        super(Listener, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        self.pblstm = nn.Sequential(\n",
    "            BLSTM(\n",
    "                input_dim=self.input_dim,\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                dropout=dropout,\n",
    "                n_layers=n_layers,\n",
    "                bidirectional = self.bidirectional\n",
    "            ),\n",
    "            BLSTM(\n",
    "                input_dim=self.hidden_dim << 1 if self.bidirectional else 0,\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                dropout=dropout,\n",
    "                n_layers=n_layers,\n",
    "                bidirectional = self.bidirectional\n",
    "            ),\n",
    "            BLSTM(\n",
    "                input_dim=self.hidden_dim << 1 if self.bidirectional else 0,\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                dropout=dropout,\n",
    "                n_layers=n_layers,\n",
    "                bidirectional = self.bidirectional\n",
    "            ))\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = Variable(torch.zeros(self.n_layers * 2 if self.bidirectional else 1, batch_size, self.hidden_dim))\n",
    "        cell = Variable(torch.zeros(self.n_layers * 2 if self.bidirectional else 1, batch_size, self.hidden_dim))\n",
    "        return (hidden.cuda(),cell.cuda())\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        hc = self.init_hidden(inputs.size(0))\n",
    "        listener_features, _ = self.pblstm((inputs, hc))\n",
    "\n",
    "        return listener_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, query, value):\n",
    "        score = torch.bmm(query, value.transpose(1, 2)) / np.sqrt(self.dim)\n",
    "        attn = F.softmax(score, dim=-1)\n",
    "        context = torch.bmm(attn, value)\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = int(hidden_dim / num_heads)\n",
    "        self.scaled_dot = ScaledDotProductAttention(self.dim)\n",
    "        self.query_projection = nn.Linear(hidden_dim, self.dim * num_heads)\n",
    "        self.value_projection = nn.Linear(hidden_dim, self.dim * num_heads)\n",
    "        self.out_projection = nn.Linear(hidden_dim << 1, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, query, value, prev_attn=None):\n",
    "        batch_size = value.size(0)\n",
    "        residual = query\n",
    "\n",
    "        query = self.query_projection(query).view(batch_size, -1, self.num_heads, self.dim)\n",
    "        value = self.value_projection(value).view(batch_size, -1, self.num_heads, self.dim)\n",
    "\n",
    "        query = query.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.dim)\n",
    "        value = value.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.dim)\n",
    "\n",
    "        context, attn = self.scaled_dot(query, value)\n",
    "        context = context.view(self.num_heads, batch_size, -1, self.dim)\n",
    "\n",
    "        context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.dim)\n",
    "        combined = torch.cat([context, residual], dim=2)\n",
    "\n",
    "        output = torch.tanh(self.out_projection(combined.view(-1, self.hidden_dim << 1))).view(batch_size, -1, self.hidden_dim)\n",
    "        return output, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speller(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim, max_step=300, sos_token=1, eos_token=2, dropout=0.0, n_layers=2, num_heads=4):\n",
    "        super(Speller, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.max_step = max_step\n",
    "        self.eos_token = eos_token\n",
    "        self.sos_token = sos_token\n",
    "        \n",
    "        #self.blendding = nn.Linear(self.hidden_dim<<1, self.hidden_dim)\n",
    "        self.emb = nn.Embedding(self.num_classes, self.hidden_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.n_layers,\n",
    "            bidirectional = False,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.init_rnn_weights()\n",
    "        self.attention = MultiHeadAttention(self.hidden_dim, self.num_heads)\n",
    "        #self.attention = Attention(dec_dim=self.hidden_dim, enc_dim=self.hidden_dim, conv_dim=1, attn_dim=self.hidden_dim)\n",
    "        self.character_distribution = nn.Linear(self.hidden_dim, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def init_rnn_weights(self, low=-0.1, high=0.1):\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.uniform_(param.data, a=low, b=high)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.uniform_(param.data, a=low, b=high)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "        cell = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "        return (hidden.cuda(),cell.cuda())\n",
    "    \n",
    "    def forward_step(self,inputs, hc, listener_features):\n",
    "        decoder_output, hc = self.rnn(inputs, hc)\n",
    "        att_out, context = self.attention(decoder_output,listener_features)\n",
    "        #context = torch.sum(att_out, dim=1).unsqueeze(dim=1)\n",
    "        #concat_output = torch.cat((decoder_output,context),dim=-1)\n",
    "        logit = self.softmax(self.character_distribution(att_out))\n",
    "\n",
    "        return logit, hc, context\n",
    "    \n",
    "    def forward(self, listener_features, ground_truth=None, teacher_forcing_rate = 0.9, use_beam=False, beam_size=3):\n",
    "        if ground_truth is None:\n",
    "            teacher_forcing_rate = 0\n",
    "        teacher_forcing = True if np.random.random_sample() < teacher_forcing_rate else False\n",
    "        \n",
    "        if (ground_truth is None) and (not teacher_forcing):\n",
    "            max_step = self.max_step\n",
    "        else:\n",
    "            max_step = ground_truth.size(1)-1\n",
    "        \n",
    "        input_word = torch.zeros(listener_features.size(0), 1).long().cuda()\n",
    "        input_word[:,0] = self.sos_token\n",
    "        \n",
    "        init_context = torch.zeros_like(listener_features[:,0:1,:])\n",
    "        inputs = self.emb(input_word)\n",
    "        hc = self.init_hidden(input_word.size(0))\n",
    "        logits = []\n",
    "        \n",
    "        if not use_beam:\n",
    "            for step in range(max_step):\n",
    "                logit, hc, context = self.forward_step(inputs, hc, listener_features)\n",
    "                logits.append(logit.squeeze())\n",
    "                if teacher_forcing:\n",
    "                    output_word = ground_truth[:,step+1:step+2]\n",
    "                else:\n",
    "                    output_word = logit.topk(1)[1].squeeze(-1)\n",
    "                inputs = self.emb(output_word)\n",
    "\n",
    "            logits = torch.stack(logits, dim=1)\n",
    "            #y_hats = torch.max(logits, dim=-1)[1]\n",
    "            return logits\n",
    "        else:\n",
    "            btz = listener_features.size(0)\n",
    "            y_hats = torch.zeros(btz, max_step).long().cuda()\n",
    "            logit, hc, context = self.forward_step(inputs, hc, listener_features)\n",
    "            output_words = logit.topk(beam_size)[1].squeeze(1)\n",
    "            for bi in range(btz):\n",
    "                b_output_words = output_words[bi,:].unsqueeze(0).transpose(1,0).contiguous()\n",
    "                b_inputs = self.emb(b_output_words)\n",
    "                b_listener_features = listener_features[bi,:,:].unsqueeze(0).expand((beam_size,-1,-1)).contiguous()\n",
    "                if isinstance(hc, tuple):\n",
    "                    b_h = hc[0][:,bi,:].unsqueeze(1).expand((-1,beam_size,-1)).contiguous()\n",
    "                    b_c = hc[1][:,bi,:].unsqueeze(1).expand((-1,beam_size,-1)).contiguous()\n",
    "                    b_hc = (b_h, b_c)\n",
    "                else:\n",
    "                    b_hc = hc[:,bi,:].unsqueeze(1).expand((-1,beam_size,-1)).contiguous()\n",
    "                    \n",
    "                scores = torch.zeros(beam_size,1).cuda()\n",
    "                ids = torch.zeros(beam_size, max_step, 1).long().cuda()\n",
    "                for step in range(max_step):\n",
    "                    logit, b_hc, context = self.forward_step(b_inputs, b_hc, b_listener_features)\n",
    "                    score, id = logit.topk(1)\n",
    "                    scores += score.squeeze(1)\n",
    "                    ids[:,step,:] = id.squeeze(1)\n",
    "                    output_word = logit.topk(1)[1].squeeze(-1)\n",
    "                    b_inputs = self.emb(output_word)\n",
    "                #print(scores.squeeze(1).topk(1)[1])\n",
    "                y_hats[bi,:] = ids[scores.squeeze(1).topk(1)[1],:].squeeze(2)\n",
    "            return y_hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAS(nn.Module):\n",
    "    def __init__(self, listener, speller):\n",
    "        super(LAS, self).__init__()\n",
    "        self.listener = listener\n",
    "        self.speller = speller\n",
    "        \n",
    "    def forward(self, inputs, ground_truth=None, teacher_forcing_rate=0.9, use_beam=False, beam_size=16):\n",
    "        listener_features = self.listener(inputs)\n",
    "        logits = self.speller(listener_features, ground_truth, \n",
    "                              teacher_forcing_rate=teacher_forcing_rate, use_beam=use_beam, beam_size=beam_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/aihub/uchar2id.pkl', 'rb') as f:\n",
    "    char2id = pickle.load(f)\n",
    "    \n",
    "with open('data/aihub/uid2char.pkl', 'rb') as f:\n",
    "    id2char = pickle.load(f)\n",
    "\n",
    "PAD_TOKEN = int(char2id['<pad>'])\n",
    "SOS_TOKEN = int(char2id['<sos>'])\n",
    "EOS_TOKEN = int(char2id['<eos>'])\n",
    "UNK_TOKEN = int(char2id['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/aihub/train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    \n",
    "with open('data/aihub/test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchaudio\n",
    "from torchaudio import transforms, functional\n",
    "import torch.nn.functional as F\n",
    "from specaugment.spec_augment_pytorch import spec_augment\n",
    "import random\n",
    "\n",
    "class SpeechDataset(Dataset):\n",
    "\n",
    "    def __init__(self, pair_data, char2id, max_len=0, specaugment=False, pkwargs=None):\n",
    "        \n",
    "        super(SpeechDataset, self).__init__()\n",
    "        self.pair_data = list(pair_data)\n",
    "        self.char2id = char2id\n",
    "        self.max_len = max_len\n",
    "        self.specaugment = specaugment\n",
    "        self.pkwargs = pkwargs\n",
    "        if self.specaugment:\n",
    "            self.origin_data = list(self.pair_data)\n",
    "            self.aug_ids = [id for id in range(len(self.pair_data), len(self.pair_data)*2)]\n",
    "            self.pair_data.extend(self.pair_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pair_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        audio_path, label = self.pair_data[idx]\n",
    "        if audio_path.split('.')[-1] == 'pcm':\n",
    "            #pcm = np.memmap(audio_path, dtype='h', mode='r')\n",
    "            #audio = torch.FloatTensor(pcm).unsqueeze(0)\n",
    "            audio = np.memmap(audio_path, dtype='h', mode='r')\n",
    "            audio = audio.astype('float32') / 32767\n",
    "\n",
    "            audio = torch.from_numpy(audio).type(torch.FloatTensor)\n",
    "        else:\n",
    "            audio, _ = torchaudio.load(audio_path)\n",
    "            \n",
    "        #audio = self.trim(audio)\n",
    "        #x = self.log_scale(self.transform(audio)).unsqueeze(0)\n",
    "        \"\"\"\n",
    "        spectrogram = torch.stft(\n",
    "            signal,\n",
    "            self.pkwargs['n_fft'],\n",
    "            hop_length=self.pkwargs['hop_length'],\n",
    "            win_length=self.pkwargs['n_fft'],\n",
    "            window=torch.hamming_window(self.n_fft),\n",
    "            center=False,\n",
    "            normalized=False,\n",
    "            onesided=True\n",
    "        )\n",
    "        \"\"\"\n",
    "        x = torchaudio.compliance.kaldi.fbank(audio.unsqueeze(0), num_mel_bins=80).t().unsqueeze(0)\n",
    "        if self.specaugment:\n",
    "            if idx in self.aug_ids:\n",
    "                x = spec_augment(x, time_warping_para=40, frequency_masking_para=13,\n",
    "                                time_masking_para=30, frequency_mask_num=2, time_mask_num=2)\n",
    "        x = x[0,:,:].squeeze(1).t()\n",
    "        if self.max_len:\n",
    "            x = np.pad(x, ((0, 0), (0, self.max_len - x.shape[1])), \"constant\")\n",
    "            \n",
    "        y = []\n",
    "        y.append(SOS_TOKEN)\n",
    "        for char in label:\n",
    "            try:\n",
    "                y.append(self.char2id[char])\n",
    "            except:\n",
    "                y.append(self.char2id['<unk>'])\n",
    "        y.append(EOS_TOKEN)\n",
    "        y = np.array(y)\n",
    "        return (x, y)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.pair_data = list(self.origin_data)\n",
    "        random.shuffle(self.pair_data)\n",
    "        if self.specaugment:\n",
    "            self.aug_ids = [id for id in range(len(self.pair_data), len(self.pair_data)*2)]\n",
    "            self.pair_data.extend(self.pair_data)\n",
    "            \n",
    "    def trim(self, sig, hop_size=64, threshhold=0.002):\n",
    "        head = None\n",
    "        tail = None\n",
    "        #threshhold = ((sig.max())/2)*threshhold\n",
    "        sig_len = len(sig)\n",
    "        for i in range(int(sig_len/hop_size)):\n",
    "            pre = sig[i*hop_size:(i+1)*hop_size].abs().sum().item()\n",
    "            post = sig[(i+1)*hop_size:(i+2)*hop_size].abs().sum().item()\n",
    "            grad = abs((post-pre)/hop_size)\n",
    "            if grad>threshhold:\n",
    "                head = (i+1)*hop_size\n",
    "                break\n",
    "\n",
    "        for i in range(int(sig_len/hop_size)):\n",
    "            pre = sig[sig_len-(i+1)*hop_size:sig_len-i*hop_size].abs().sum().item()\n",
    "            post = sig[sig_len-(i+2)*hop_size:sig_len-(i+1)*hop_size].abs().sum().item()\n",
    "            grad = abs((post-pre)/hop_size)\n",
    "            if grad>threshhold:\n",
    "                tail = sig_len-(i+1)*hop_size\n",
    "                break\n",
    "        #print(head, tail)\n",
    "        return sig[head:tail]\n",
    "\n",
    "\"\"\"\n",
    "def pad(batch):\n",
    "    global transform, do_normalize\n",
    "    max_len_x = 0\n",
    "    max_len_y = 0\n",
    "    data = []\n",
    "    target = []\n",
    "    for sample in batch:\n",
    "        data += [sample[0]]\n",
    "        target += [sample[1]]\n",
    "        n = sample[0].shape[1]\n",
    "        m = sample[1].shape[0]\n",
    "        if max_len_x < n:\n",
    "            max_len_x = n\n",
    "        if max_len_y < m:\n",
    "            max_len_y = m\n",
    "    data = torch.tensor([F.pad(input=x, pad=(0, max_len_x - x.shape[1])).numpy() for x in data])\n",
    "    target = [np.pad(y, (0, max_len_y - y.shape[0]), 'constant') for y in target]\n",
    "    \n",
    "    return data.contiguous(), torch.tensor(target).contiguous()\n",
    "\"\"\"\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    \"\"\" functions that pad to the maximum sequence length \"\"\"\n",
    "    def seq_length_(p):\n",
    "        return len(p[0])\n",
    "\n",
    "    def target_length_(p):\n",
    "        return len(p[1])\n",
    "\n",
    "    seq_lengths = [len(s[0]) for s in batch]\n",
    "    target_lengths = [len(s[1]) for s in batch]\n",
    "\n",
    "    max_seq_sample = max(batch, key=seq_length_)[0]\n",
    "    max_target_sample = max(batch, key=target_length_)[1]\n",
    "\n",
    "    max_seq_size = max_seq_sample.size(0)\n",
    "    max_target_size = len(max_target_sample)\n",
    "\n",
    "    feat_size = max_seq_sample.size(1)\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    seqs = torch.zeros(batch_size, max_seq_size, feat_size)\n",
    "\n",
    "    targets = torch.zeros(batch_size, max_target_size).to(torch.long)\n",
    "    targets.fill_(0)\n",
    "\n",
    "    for x in range(batch_size):\n",
    "        sample = batch[x]\n",
    "        tensor = sample[0]\n",
    "        target = sample[1]\n",
    "        seq_length = tensor.size(0)\n",
    "        seqs[x].narrow(0, 0, seq_length).copy_(tensor)\n",
    "        targets[x].narrow(0, 0, len(target)).copy_(torch.LongTensor(target))\n",
    "    return seqs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "num_channels = 80\n",
    "train_dataset = SpeechDataset(train_data, char2id, specaugment=True, max_len=0)\n",
    "test_dataset = SpeechDataset(test_data, char2id, max_len=0)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                              collate_fn=_collate_fn, num_workers=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                             collate_fn=_collate_fn, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as Lev\n",
    "\n",
    "\n",
    "def WER(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))\n",
    "\n",
    "\n",
    "def CER(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "    s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
    "    return Lev.distance(s1, s2)\n",
    "\n",
    "def label_to_string(labels, id2char):\n",
    "    \"\"\"\n",
    "    Converts label to string (number => Hangeul)\n",
    "\n",
    "    Args:\n",
    "        labels (list): number label\n",
    "        id2char (dict): id2char[id] = ch\n",
    "        eos_id (int): identification of <end of sequence>\n",
    "\n",
    "    Returns: sentence\n",
    "        - **sentence** (str or list): Hangeul representation of labels\n",
    "    \"\"\"\n",
    "    sos_id = id2char.index('<sos>')\n",
    "    eos_id = id2char.index('<eos>')\n",
    "    #unk_id = id2char.index('<unk>')\n",
    "    if len(labels.shape) == 1:\n",
    "        sentence = str()\n",
    "        for label in labels:\n",
    "            if label.item() == sos_id:\n",
    "                continue\n",
    "            if label.item() == eos_id:\n",
    "                break\n",
    "            sentence += id2char[label.item()]\n",
    "        return sentence\n",
    "\n",
    "    elif len(labels.shape) == 2:\n",
    "        sentences = list()\n",
    "        for batch in labels:\n",
    "            sentence = str()\n",
    "            for label in batch:\n",
    "                if label.item() == sos_id:\n",
    "                    continue\n",
    "                if label.item() == eos_id:\n",
    "                    break\n",
    "                sentence += id2char[label.item()]\n",
    "            sentences.append(sentence)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, vocab_size, ignore_index, smoothing=0.1, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        with torch.no_grad():\n",
    "            label_smoothed = torch.zeros_like(logit)\n",
    "            label_smoothed.fill_(self.smoothing / (self.vocab_size - 1))\n",
    "            #print(label_smoothed, target.data.unsqueeze(1))\n",
    "            label_smoothed.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "            label_smoothed[target == self.ignore_index, :] = 0\n",
    "        return torch.sum(-label_smoothed * logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "listener = Listener(num_channels, 256)\n",
    "speller = Speller(len(id2char), 512, num_heads=4, dropout=0.3)\n",
    "model = LAS(listener, speller).cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "#optimizer = optim.ASGD(model.parameters(), lr=3e-4)\n",
    "criterion = LabelSmoothingLoss(len(char2id), ignore_index = PAD_TOKEN, smoothing = 0.1, dim = -1).cuda()\n",
    "#criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=PAD_TOKEN).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncheckpoint = torch.load('checkpoint/aihub-3/last_checkpoint.pt')\\nmodel.load_state_dict(checkpoint['model_state_dict'])\\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\\nepoch = checkpoint['epoch']\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "checkpoint = torch.load('checkpoint/aihub-3/last_checkpoint.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_checkpoint(path, epoch, model, optimizer, loss):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path+'/last_checkpoint.pt')\n",
    "    \n",
    "def score(pred, y):\n",
    "    cer = 0.\n",
    "    wer = 0.\n",
    "    pred_sts = label_to_string(pred, id2char)\n",
    "    gold_sts = label_to_string(y, id2char)\n",
    "    for i, (res,gt) in enumerate(zip(pred_sts,gold_sts)):\n",
    "        c_length = len(gt.replace(' ',''))\n",
    "        cer += CER(res,gt)/c_length\n",
    "        \n",
    "        w_length = len(gt.split())\n",
    "        wer += WER(res,gt)/w_length\n",
    "    return cer, wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler_sampling(epoch, e_min=5, ratio_s=0.9, ratio_e=0, n_epoch_ramp=10):\n",
    "    if epoch>e_min:\n",
    "        epoch -= e_min\n",
    "        teacher_forcing_ratio = max(ratio_s - (ratio_s-ratio_e)*epoch/n_epoch_ramp, ratio_e)\n",
    "    else:\n",
    "        teacher_forcing_ratio = 0.9\n",
    "    return teacher_forcing_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(optimizer, lr):\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:    0/77661, loss: 145.7933, cer: 7.19, wer: 1.00, tf_rate: 0.90\n",
      "timestep:   50/77661, loss: 180.5205, cer: 1.20, wer: 1.00, tf_rate: 0.90\n",
      "timestep:  100/77661, loss: 173.5473, cer: 1.13, wer: 1.12, tf_rate: 0.90\n",
      "timestep:  150/77661, loss: 167.2752, cer: 1.11, wer: 1.18, tf_rate: 0.90\n",
      "timestep:  200/77661, loss: 163.2152, cer: 1.07, wer: 1.13, tf_rate: 0.90\n",
      "timestep:  250/77661, loss: 159.5642, cer: 1.04, wer: 1.11, tf_rate: 0.90\n",
      "timestep:  300/77661, loss: 156.9817, cer: 1.01, wer: 1.09, tf_rate: 0.90\n",
      "timestep:  350/77661, loss: 154.1604, cer: 1.00, wer: 1.09, tf_rate: 0.90\n",
      "timestep:  400/77661, loss: 153.4419, cer: 0.98, wer: 1.08, tf_rate: 0.90\n",
      "timestep:  450/77661, loss: 152.6465, cer: 0.97, wer: 1.07, tf_rate: 0.90\n",
      "timestep:  500/77661, loss: 151.5509, cer: 0.96, wer: 1.07, tf_rate: 0.90\n",
      "timestep:  550/77661, loss: 149.7598, cer: 0.95, wer: 1.07, tf_rate: 0.90\n",
      "timestep:  600/77661, loss: 148.8346, cer: 0.94, wer: 1.06, tf_rate: 0.90\n",
      "timestep:  650/77661, loss: 147.8108, cer: 0.94, wer: 1.06, tf_rate: 0.90\n",
      "timestep:  700/77661, loss: 146.3809, cer: 0.93, wer: 1.06, tf_rate: 0.90\n",
      "timestep:  750/77661, loss: 145.6981, cer: 0.92, wer: 1.05, tf_rate: 0.90\n",
      "timestep:  800/77661, loss: 144.5548, cer: 0.92, wer: 1.05, tf_rate: 0.90\n",
      "timestep:  850/77661, loss: 144.2460, cer: 0.92, wer: 1.05, tf_rate: 0.90\n",
      "timestep:  900/77661, loss: 143.4029, cer: 0.91, wer: 1.05, tf_rate: 0.90\n",
      "timestep:  950/77661, loss: 142.8246, cer: 0.91, wer: 1.05, tf_rate: 0.90\n",
      "timestep: 1000/77661, loss: 141.7966, cer: 0.91, wer: 1.05, tf_rate: 0.90\n",
      "timestep: 1050/77661, loss: 141.3430, cer: 0.90, wer: 1.05, tf_rate: 0.90\n",
      "timestep: 1100/77661, loss: 141.0441, cer: 0.90, wer: 1.05, tf_rate: 0.90\n",
      "timestep: 1150/77661, loss: 140.5916, cer: 0.90, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1200/77661, loss: 140.2891, cer: 0.89, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1250/77661, loss: 140.0555, cer: 0.89, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1300/77661, loss: 139.6120, cer: 0.89, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1350/77661, loss: 139.0380, cer: 0.88, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1400/77661, loss: 138.7118, cer: 0.88, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1450/77661, loss: 138.5913, cer: 0.88, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1500/77661, loss: 138.2140, cer: 0.88, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1550/77661, loss: 137.8836, cer: 0.88, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1600/77661, loss: 137.6757, cer: 0.88, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1650/77661, loss: 137.5442, cer: 0.87, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1700/77661, loss: 137.4287, cer: 0.87, wer: 1.04, tf_rate: 0.90\n",
      "timestep: 1750/77661, loss: 137.2575, cer: 0.87, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 1800/77661, loss: 136.9612, cer: 0.87, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 1850/77661, loss: 136.7589, cer: 0.87, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 1900/77661, loss: 136.3472, cer: 0.87, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 1950/77661, loss: 136.0006, cer: 0.87, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2000/77661, loss: 135.8087, cer: 0.86, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2050/77661, loss: 135.5013, cer: 0.86, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2100/77661, loss: 135.1846, cer: 0.86, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2150/77661, loss: 135.0502, cer: 0.86, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2200/77661, loss: 134.8446, cer: 0.86, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2250/77661, loss: 134.7874, cer: 0.86, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2300/77661, loss: 134.5373, cer: 0.86, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2350/77661, loss: 134.2144, cer: 0.86, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2400/77661, loss: 134.0976, cer: 0.86, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2450/77661, loss: 133.9775, cer: 0.86, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2500/77661, loss: 133.8857, cer: 0.85, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2550/77661, loss: 133.7152, cer: 0.85, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2600/77661, loss: 133.5850, cer: 0.85, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2650/77661, loss: 133.3544, cer: 0.85, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2700/77661, loss: 133.2116, cer: 0.85, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2750/77661, loss: 133.4704, cer: 0.85, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2800/77661, loss: 133.2821, cer: 0.85, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2850/77661, loss: 133.1386, cer: 0.85, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2900/77661, loss: 133.0878, cer: 0.85, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 2950/77661, loss: 132.8709, cer: 0.85, wer: 1.03, tf_rate: 0.90\n",
      "timestep: 3000/77661, loss: 132.6670, cer: 0.85, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3050/77661, loss: 132.5223, cer: 0.85, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3100/77661, loss: 132.3390, cer: 0.85, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3150/77661, loss: 132.1887, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3200/77661, loss: 132.0601, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3250/77661, loss: 131.9162, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3300/77661, loss: 131.6877, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3350/77661, loss: 131.5701, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3400/77661, loss: 131.4356, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3450/77661, loss: 131.2215, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3500/77661, loss: 131.0725, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3550/77661, loss: 130.8169, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3600/77661, loss: 130.8279, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3650/77661, loss: 130.7289, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3700/77661, loss: 130.6706, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3750/77661, loss: 130.5962, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3800/77661, loss: 130.4563, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3850/77661, loss: 130.3035, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3900/77661, loss: 130.2160, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 3950/77661, loss: 130.0998, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4000/77661, loss: 129.9780, cer: 0.84, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4050/77661, loss: 129.8686, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4100/77661, loss: 129.7626, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4150/77661, loss: 129.6169, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4200/77661, loss: 129.5688, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4250/77661, loss: 129.5037, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4300/77661, loss: 129.4263, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4350/77661, loss: 129.3252, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4400/77661, loss: 129.2311, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4450/77661, loss: 129.1338, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4500/77661, loss: 128.9950, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4550/77661, loss: 128.8785, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4600/77661, loss: 128.8104, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4650/77661, loss: 128.7320, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4700/77661, loss: 128.7231, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4750/77661, loss: 128.6157, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4800/77661, loss: 128.6139, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4850/77661, loss: 128.5751, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4900/77661, loss: 128.5722, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 4950/77661, loss: 128.4801, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 5000/77661, loss: 128.4213, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 5050/77661, loss: 128.4212, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 5100/77661, loss: 128.3049, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 5150/77661, loss: 128.2079, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 5200/77661, loss: 128.2695, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 5250/77661, loss: 128.2931, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 5300/77661, loss: 128.2864, cer: 0.83, wer: 1.02, tf_rate: 0.90\n",
      "timestep: 5350/77661, loss: 128.2249, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5400/77661, loss: 128.1737, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5450/77661, loss: 128.0994, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5500/77661, loss: 127.9940, cer: 0.82, wer: 1.01, tf_rate: 0.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 5550/77661, loss: 127.9559, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5600/77661, loss: 127.9134, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5650/77661, loss: 127.7805, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5700/77661, loss: 127.7302, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5750/77661, loss: 127.6622, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5800/77661, loss: 127.5743, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5850/77661, loss: 127.5810, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5900/77661, loss: 127.5753, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 5950/77661, loss: 127.5845, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6000/77661, loss: 127.5700, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6050/77661, loss: 127.5033, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6100/77661, loss: 127.4560, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6150/77661, loss: 127.3824, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6200/77661, loss: 127.3549, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6250/77661, loss: 127.3465, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6300/77661, loss: 127.2987, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6350/77661, loss: 127.2421, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6400/77661, loss: 127.1718, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6450/77661, loss: 127.1123, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6500/77661, loss: 127.0107, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6550/77661, loss: 126.8760, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6600/77661, loss: 126.8386, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6650/77661, loss: 126.7993, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6700/77661, loss: 126.7424, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6750/77661, loss: 126.6731, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6800/77661, loss: 126.6075, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6850/77661, loss: 126.5206, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6900/77661, loss: 126.4627, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 6950/77661, loss: 126.4100, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7000/77661, loss: 126.3595, cer: 0.82, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7050/77661, loss: 126.3346, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7100/77661, loss: 126.2327, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7150/77661, loss: 126.2012, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7200/77661, loss: 126.1616, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7250/77661, loss: 126.1273, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7300/77661, loss: 126.0818, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7350/77661, loss: 126.0995, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7400/77661, loss: 126.0844, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7450/77661, loss: 126.0581, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7500/77661, loss: 126.0715, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7550/77661, loss: 126.0214, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7600/77661, loss: 125.9796, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7650/77661, loss: 125.9325, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7700/77661, loss: 125.8789, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7750/77661, loss: 125.8373, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7800/77661, loss: 125.7831, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7850/77661, loss: 125.7586, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7900/77661, loss: 125.7484, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 7950/77661, loss: 125.7768, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 8000/77661, loss: 125.6851, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 8050/77661, loss: 125.6401, cer: 0.81, wer: 1.01, tf_rate: 0.90\n",
      "timestep: 8100/77661, loss: 125.6637, cer: 0.81, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8150/77661, loss: 125.6196, cer: 0.81, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8200/77661, loss: 125.5842, cer: 0.81, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8250/77661, loss: 125.5108, cer: 0.81, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8300/77661, loss: 125.4978, cer: 0.81, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8350/77661, loss: 125.4411, cer: 0.81, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8400/77661, loss: 125.3793, cer: 0.81, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8450/77661, loss: 125.2941, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8500/77661, loss: 125.2742, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8550/77661, loss: 125.2435, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8600/77661, loss: 125.1699, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8650/77661, loss: 125.1182, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8700/77661, loss: 125.0703, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8750/77661, loss: 124.9853, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8800/77661, loss: 124.9472, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8850/77661, loss: 124.8528, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8900/77661, loss: 124.7659, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 8950/77661, loss: 124.7235, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9000/77661, loss: 124.6748, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9050/77661, loss: 124.6138, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9100/77661, loss: 124.5762, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9150/77661, loss: 124.5342, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9200/77661, loss: 124.4425, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9250/77661, loss: 124.3991, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9300/77661, loss: 124.3290, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9350/77661, loss: 124.2842, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9400/77661, loss: 124.2450, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9450/77661, loss: 124.2218, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9500/77661, loss: 124.1536, cer: 0.80, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9550/77661, loss: 124.1222, cer: 0.79, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9600/77661, loss: 124.0745, cer: 0.79, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9650/77661, loss: 124.0223, cer: 0.79, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9700/77661, loss: 123.9669, cer: 0.79, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9750/77661, loss: 123.9155, cer: 0.79, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9800/77661, loss: 123.8701, cer: 0.79, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9850/77661, loss: 123.9240, cer: 0.79, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9900/77661, loss: 123.8903, cer: 0.79, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 9950/77661, loss: 123.8208, cer: 0.79, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 10000/77661, loss: 123.8074, cer: 0.79, wer: 1.00, tf_rate: 0.90\n",
      "timestep: 10050/77661, loss: 123.7687, cer: 0.79, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10100/77661, loss: 123.6862, cer: 0.79, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10150/77661, loss: 123.6078, cer: 0.79, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10200/77661, loss: 123.5234, cer: 0.79, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10250/77661, loss: 123.4711, cer: 0.79, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10300/77661, loss: 123.4079, cer: 0.79, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10350/77661, loss: 123.3639, cer: 0.79, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10400/77661, loss: 123.3273, cer: 0.79, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10450/77661, loss: 123.2730, cer: 0.79, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10500/77661, loss: 123.2031, cer: 0.79, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10550/77661, loss: 123.1455, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10600/77661, loss: 123.0821, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10650/77661, loss: 123.0373, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10700/77661, loss: 122.9910, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10750/77661, loss: 122.8773, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10800/77661, loss: 122.8254, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10850/77661, loss: 122.8069, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10900/77661, loss: 122.7715, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 10950/77661, loss: 122.7052, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11000/77661, loss: 122.6251, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11050/77661, loss: 122.5717, cer: 0.78, wer: 0.99, tf_rate: 0.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 11100/77661, loss: 122.5346, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11150/77661, loss: 122.4729, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11200/77661, loss: 122.4100, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11250/77661, loss: 122.3351, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11300/77661, loss: 122.2965, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11350/77661, loss: 122.2473, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11400/77661, loss: 122.1725, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11450/77661, loss: 122.1239, cer: 0.78, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11500/77661, loss: 122.0430, cer: 0.77, wer: 0.99, tf_rate: 0.90\n",
      "timestep: 11550/77661, loss: 121.9891, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 11600/77661, loss: 121.9349, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 11650/77661, loss: 121.8434, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 11700/77661, loss: 121.7864, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 11750/77661, loss: 121.7666, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 11800/77661, loss: 121.7234, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 11850/77661, loss: 121.6843, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 11900/77661, loss: 121.6390, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 11950/77661, loss: 121.5701, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12000/77661, loss: 121.5014, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12050/77661, loss: 121.4104, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12100/77661, loss: 121.3336, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12150/77661, loss: 121.2658, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12200/77661, loss: 121.2167, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12250/77661, loss: 121.1684, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12300/77661, loss: 121.1204, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12350/77661, loss: 121.0368, cer: 0.77, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12400/77661, loss: 120.9571, cer: 0.76, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12450/77661, loss: 120.8677, cer: 0.76, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12500/77661, loss: 120.8331, cer: 0.76, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12550/77661, loss: 120.7874, cer: 0.76, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12600/77661, loss: 120.7241, cer: 0.76, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12650/77661, loss: 120.6523, cer: 0.76, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12700/77661, loss: 120.6072, cer: 0.76, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12750/77661, loss: 120.5647, cer: 0.76, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12800/77661, loss: 120.4929, cer: 0.76, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12850/77661, loss: 120.3704, cer: 0.76, wer: 0.98, tf_rate: 0.90\n",
      "timestep: 12900/77661, loss: 120.3223, cer: 0.76, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 12950/77661, loss: 120.2686, cer: 0.76, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13000/77661, loss: 120.2178, cer: 0.76, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13050/77661, loss: 120.1505, cer: 0.76, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13100/77661, loss: 120.0886, cer: 0.76, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13150/77661, loss: 119.9952, cer: 0.76, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13200/77661, loss: 119.9318, cer: 0.76, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13250/77661, loss: 119.8826, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13300/77661, loss: 119.8352, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13350/77661, loss: 119.7297, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13400/77661, loss: 119.6849, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13450/77661, loss: 119.6150, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13500/77661, loss: 119.5406, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13550/77661, loss: 119.5009, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13600/77661, loss: 119.4111, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13650/77661, loss: 119.3383, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13700/77661, loss: 119.2494, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13750/77661, loss: 119.1644, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13800/77661, loss: 119.0892, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13850/77661, loss: 119.0193, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13900/77661, loss: 118.9504, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 13950/77661, loss: 118.8804, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 14000/77661, loss: 118.8041, cer: 0.75, wer: 0.97, tf_rate: 0.90\n",
      "timestep: 14050/77661, loss: 118.7261, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14100/77661, loss: 118.6504, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14150/77661, loss: 118.5796, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14200/77661, loss: 118.5543, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14250/77661, loss: 118.4777, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14300/77661, loss: 118.3942, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14350/77661, loss: 118.3055, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14400/77661, loss: 118.2311, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14450/77661, loss: 118.1601, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14500/77661, loss: 118.0893, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14550/77661, loss: 118.0399, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14600/77661, loss: 117.9703, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14650/77661, loss: 117.9120, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14700/77661, loss: 117.8464, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14750/77661, loss: 117.7578, cer: 0.74, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14800/77661, loss: 117.6668, cer: 0.73, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14850/77661, loss: 117.6083, cer: 0.73, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14900/77661, loss: 117.5325, cer: 0.73, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 14950/77661, loss: 117.4818, cer: 0.73, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 15000/77661, loss: 117.4442, cer: 0.73, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 15050/77661, loss: 117.3814, cer: 0.73, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 15100/77661, loss: 117.3263, cer: 0.73, wer: 0.96, tf_rate: 0.90\n",
      "timestep: 15150/77661, loss: 117.2320, cer: 0.73, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15200/77661, loss: 117.1523, cer: 0.73, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15250/77661, loss: 117.0937, cer: 0.73, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15300/77661, loss: 117.0390, cer: 0.73, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15350/77661, loss: 116.9721, cer: 0.73, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15400/77661, loss: 116.8839, cer: 0.73, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15450/77661, loss: 116.7939, cer: 0.73, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15500/77661, loss: 116.7065, cer: 0.73, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15550/77661, loss: 116.6449, cer: 0.73, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15600/77661, loss: 116.5784, cer: 0.72, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15650/77661, loss: 116.4770, cer: 0.72, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15700/77661, loss: 116.4137, cer: 0.72, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15750/77661, loss: 116.3316, cer: 0.72, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15800/77661, loss: 116.2383, cer: 0.72, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15850/77661, loss: 116.1681, cer: 0.72, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15900/77661, loss: 116.1118, cer: 0.72, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 15950/77661, loss: 116.0289, cer: 0.72, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 16000/77661, loss: 115.9180, cer: 0.72, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 16050/77661, loss: 115.8389, cer: 0.72, wer: 0.95, tf_rate: 0.90\n",
      "timestep: 16100/77661, loss: 115.7575, cer: 0.72, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16150/77661, loss: 115.6652, cer: 0.72, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16200/77661, loss: 115.5985, cer: 0.72, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16250/77661, loss: 115.4962, cer: 0.72, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16300/77661, loss: 115.4292, cer: 0.72, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16350/77661, loss: 115.3555, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16400/77661, loss: 115.2948, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16450/77661, loss: 115.2469, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16500/77661, loss: 115.2293, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16550/77661, loss: 115.1910, cer: 0.71, wer: 0.94, tf_rate: 0.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 16600/77661, loss: 115.1103, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16650/77661, loss: 115.0274, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16700/77661, loss: 114.9565, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16750/77661, loss: 114.8785, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16800/77661, loss: 114.7892, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16850/77661, loss: 114.6898, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16900/77661, loss: 114.6168, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 16950/77661, loss: 114.5558, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 17000/77661, loss: 114.4757, cer: 0.71, wer: 0.94, tf_rate: 0.90\n",
      "timestep: 17050/77661, loss: 114.4386, cer: 0.71, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17100/77661, loss: 114.3673, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17150/77661, loss: 114.2834, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17200/77661, loss: 114.2038, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17250/77661, loss: 114.1307, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17300/77661, loss: 114.0662, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17350/77661, loss: 113.9793, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17400/77661, loss: 113.9084, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17450/77661, loss: 113.8407, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17500/77661, loss: 113.7572, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17550/77661, loss: 113.6726, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17600/77661, loss: 113.6142, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17650/77661, loss: 113.5464, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17700/77661, loss: 113.4763, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17750/77661, loss: 113.4213, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17800/77661, loss: 113.3569, cer: 0.70, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17850/77661, loss: 113.2778, cer: 0.69, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17900/77661, loss: 113.2009, cer: 0.69, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 17950/77661, loss: 113.1123, cer: 0.69, wer: 0.93, tf_rate: 0.90\n",
      "timestep: 18000/77661, loss: 113.0579, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18050/77661, loss: 112.9875, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18100/77661, loss: 112.9036, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18150/77661, loss: 112.8502, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18200/77661, loss: 112.7661, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18250/77661, loss: 112.6965, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18300/77661, loss: 112.6061, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18350/77661, loss: 112.5430, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18400/77661, loss: 112.4848, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18450/77661, loss: 112.3815, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18500/77661, loss: 112.3142, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18550/77661, loss: 112.2399, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18600/77661, loss: 112.1712, cer: 0.69, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18650/77661, loss: 112.1158, cer: 0.68, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18700/77661, loss: 112.0202, cer: 0.68, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18750/77661, loss: 111.9611, cer: 0.68, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18800/77661, loss: 111.8838, cer: 0.68, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18850/77661, loss: 111.8294, cer: 0.68, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18900/77661, loss: 111.7751, cer: 0.68, wer: 0.92, tf_rate: 0.90\n",
      "timestep: 18950/77661, loss: 111.7115, cer: 0.68, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19000/77661, loss: 111.6576, cer: 0.68, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19050/77661, loss: 111.5761, cer: 0.68, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19100/77661, loss: 111.5001, cer: 0.68, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19150/77661, loss: 111.4326, cer: 0.68, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19200/77661, loss: 111.3680, cer: 0.68, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19250/77661, loss: 111.3014, cer: 0.68, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19300/77661, loss: 111.2365, cer: 0.68, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19350/77661, loss: 111.1533, cer: 0.68, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19400/77661, loss: 111.0962, cer: 0.68, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19450/77661, loss: 111.0405, cer: 0.67, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19500/77661, loss: 110.9908, cer: 0.67, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19550/77661, loss: 110.9280, cer: 0.67, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19600/77661, loss: 110.8658, cer: 0.67, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19650/77661, loss: 110.7896, cer: 0.67, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19700/77661, loss: 110.7060, cer: 0.67, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19750/77661, loss: 110.6499, cer: 0.67, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19800/77661, loss: 110.5865, cer: 0.67, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19850/77661, loss: 110.5269, cer: 0.67, wer: 0.91, tf_rate: 0.90\n",
      "timestep: 19900/77661, loss: 110.4639, cer: 0.67, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 19950/77661, loss: 110.3918, cer: 0.67, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20000/77661, loss: 110.3387, cer: 0.67, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20050/77661, loss: 110.2640, cer: 0.67, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20100/77661, loss: 110.2021, cer: 0.67, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20150/77661, loss: 110.1561, cer: 0.67, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20200/77661, loss: 110.0757, cer: 0.67, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20250/77661, loss: 110.0141, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20300/77661, loss: 109.9505, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20350/77661, loss: 109.8700, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20400/77661, loss: 109.7930, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20450/77661, loss: 109.7123, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20500/77661, loss: 109.6341, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20550/77661, loss: 109.5595, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20600/77661, loss: 109.5298, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20650/77661, loss: 109.4446, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20700/77661, loss: 109.3649, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20750/77661, loss: 109.2994, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20800/77661, loss: 109.2525, cer: 0.66, wer: 0.90, tf_rate: 0.90\n",
      "timestep: 20850/77661, loss: 109.1925, cer: 0.66, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 20900/77661, loss: 109.1469, cer: 0.66, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 20950/77661, loss: 109.1160, cer: 0.66, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21000/77661, loss: 109.0503, cer: 0.66, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21050/77661, loss: 108.9890, cer: 0.66, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21100/77661, loss: 108.9299, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21150/77661, loss: 108.8787, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21200/77661, loss: 108.8232, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21250/77661, loss: 108.7484, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21300/77661, loss: 108.6750, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21350/77661, loss: 108.6229, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21400/77661, loss: 108.5705, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21450/77661, loss: 108.4983, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21500/77661, loss: 108.4223, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21550/77661, loss: 108.3649, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21600/77661, loss: 108.3199, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21650/77661, loss: 108.2388, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21700/77661, loss: 108.1877, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21750/77661, loss: 108.1342, cer: 0.65, wer: 0.89, tf_rate: 0.90\n",
      "timestep: 21800/77661, loss: 108.0618, cer: 0.65, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 21850/77661, loss: 108.0085, cer: 0.65, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 21900/77661, loss: 107.9537, cer: 0.65, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 21950/77661, loss: 107.8876, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22000/77661, loss: 107.8254, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22050/77661, loss: 107.7667, cer: 0.64, wer: 0.88, tf_rate: 0.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 22100/77661, loss: 107.7072, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22150/77661, loss: 107.6431, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22200/77661, loss: 107.5776, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22250/77661, loss: 107.5277, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22300/77661, loss: 107.4647, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22350/77661, loss: 107.4170, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22400/77661, loss: 107.3494, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22450/77661, loss: 107.2908, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22500/77661, loss: 107.2331, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22550/77661, loss: 107.1894, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22600/77661, loss: 107.1274, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22650/77661, loss: 107.0627, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22700/77661, loss: 106.9991, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22750/77661, loss: 106.9375, cer: 0.64, wer: 0.88, tf_rate: 0.90\n",
      "timestep: 22800/77661, loss: 106.8881, cer: 0.64, wer: 0.87, tf_rate: 0.90\n",
      "timestep: 22850/77661, loss: 106.8369, cer: 0.63, wer: 0.87, tf_rate: 0.90\n",
      "timestep: 22900/77661, loss: 106.7782, cer: 0.63, wer: 0.87, tf_rate: 0.90\n",
      "timestep: 22950/77661, loss: 106.7215, cer: 0.63, wer: 0.87, tf_rate: 0.90\n",
      "timestep: 23000/77661, loss: 106.6654, cer: 0.63, wer: 0.87, tf_rate: 0.90\n",
      "timestep: 23050/77661, loss: 106.5941, cer: 0.63, wer: 0.87, tf_rate: 0.90\n",
      "timestep: 23100/77661, loss: 106.5499, cer: 0.63, wer: 0.87, tf_rate: 0.90\n",
      "timestep: 23150/77661, loss: 106.5043, cer: 0.63, wer: 0.87, tf_rate: 0.90\n",
      "timestep: 23200/77661, loss: 106.4673, cer: 0.63, wer: 0.87, tf_rate: 0.90\n",
      "timestep: 23250/77661, loss: 106.4159, cer: 0.63, wer: 0.87, tf_rate: 0.90\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 100\n",
    "checkpoint = 1\n",
    "print_step = 50\n",
    "save_epoch = 1\n",
    "checkpoint_path = 'checkpoint/aihub'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    num_samples = 0\n",
    "    cer = 0.\n",
    "    wer = 0.\n",
    "    step= 0\n",
    "    if epoch==5:\n",
    "        set_lr(optimizer, 1e-5)\n",
    "    total_step = len(train_dataloader)\n",
    "    for batch in train_dataloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y = batch\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        target = y[:, 1:].contiguous().cuda()\n",
    "        teacher_forcing_rate = scheduler_sampling(epoch)\n",
    "        logits = model(x, ground_truth=y, teacher_forcing_rate=teacher_forcing_rate)\n",
    "        \n",
    "        y_hats = torch.max(logits, dim=-1)[1]\n",
    "        #print(label_to_string(target, id2char))\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_samples += batch_size\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=400)\n",
    "        optimizer.step()\n",
    "        cer_, wer_ = score(y_hats.long(), target)\n",
    "        cer += cer_\n",
    "        wer += wer_\n",
    "        if step%print_step==0:\n",
    "            print('timestep: {:4d}/{:4d}, loss: {:.4f}, cer: {:.2f}, wer: {:.2f}, tf_rate: {:.2f}'.format(\n",
    "                step, total_step, total_loss/num_samples, cer/num_samples, wer/num_samples, teacher_forcing_rate))\n",
    "            with open('aihub-4.log', 'at') as f:\n",
    "                f.write('timestep: {:4d}/{:4d}, loss: {:.4f}, cer: {:.2f}, wer: {:.2f}, tf_rate: {:.2f}\\n'.format(\n",
    "                step, total_step, total_loss/num_samples, cer/num_samples, wer/num_samples, teacher_forcing_rate))\n",
    "        step += 1\n",
    "        \n",
    "    total_loss /= num_samples\n",
    "    cer /= num_samples\n",
    "    wer /= num_samples\n",
    "    print('Epoch %d (Training) Total Loss %0.4f CER %0.4f WER %0.4f' % (epoch, total_loss, cer, wer))\n",
    "    with open('aihub-4.log', 'at') as f:\n",
    "        f.write('Epoch %d (Training) Total Loss %0.4f CER %0.4f WER %0.4f\\n' % (epoch, total_loss, cer, wer))\n",
    "    train_dataloader.dataset.shuffle()\n",
    "        \n",
    "    total_loss = 0.\n",
    "    num_samples = 0\n",
    "    cer = 0.\n",
    "    wer = 0.\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in test_dataloader:\n",
    "            x, y = batch\n",
    "\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            target = y[:, 1:].contiguous().cuda()\n",
    "\n",
    "            logits = model(x, ground_truth=None, teacher_forcing_rate=0.0)\n",
    "\n",
    "            y_hats = torch.max(logits, dim=-1)[1]\n",
    "            logits = logits[:,:target.size(1),:].contiguous() # cut over length to calculate loss\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            cer_, wer_ = score(y_hats.long(), target)\n",
    "            cer += cer_\n",
    "            wer += wer_\n",
    "            num_samples += batch_size\n",
    "    val_loss = total_loss/num_samples\n",
    "    cer /= num_samples\n",
    "    wer /= num_samples\n",
    "    #scheduler.step(val_loss)\n",
    "    print('Epoch %d (Evaluate) Total Loss %0.4f CER %0.4f WER %0.4f' % (epoch, val_loss, cer, wer))\n",
    "    with open('aihub-4.log', 'at') as f:\n",
    "        f.write('Epoch %d (Evaluate) Total Loss %0.4f CER %0.4f WER %0.4f\\n' % (epoch, val_loss, cer, wer))\n",
    "    last_checkpoint(checkpoint_path+'/', epoch, model, optimizer, loss)\n",
    "    if epoch%save_epoch==0:\n",
    "        torch.save(model, \"{}/epoch{}-cer{:.2f}-wer{:.2f}.pt\".format(checkpoint_path, epoch, cer, wer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
